"""
Dynamic Dependencies Bridge for JPS-to-Bazel target generation (community version)

HOW TO ADD BUILD-ON-DEMAND TO YOUR PROJECT:

If your project requires pre-build of the whole Ultimate, just add this load statement
load("@jps_dynamic_deps_community//:targets.bzl", "ALL_COMMUNITY_TARGETS")
to the BUILD.bazel file with your target and pass ALL_COMMUNITY_TARGETS to "data" field
(alternatively use ALL_PRODUCTION_COMMUNITY_TARGETS if you don't need test_lib targets)

You need to pass it to data, and not dependencies, because you only need to trigger the build.
The actual outputs aren't passed through Bazel, but instead discovered through build/bazel-targets.json file.

PROBLEM:

The list of project targets is generated by a script (jps-to-bazel.cmd) that converts
JPS module definitions to Bazel targets. However, Bazel's "Loading Phase" happens *before*
any build action runs. On a fresh clone or 'git clean -xdf', the `build/bazel-targets.json`
file does not exist, and Bazel cannot load targets directly.

SOLUTION:

We use a Repository Rule that runs the JPS-to-Bazel converter during repository initialization,
before the loading phase completes. This ensures that target information is always available.

The repository rule:
1. Executes jps-to-bazel.cmd to generate/update bazel-targets.json
2. Parses the JSON file and extracts productionTargets and testTargets from each module
3. Generates targets.bzl with exported lists:
   - ALL_PRODUCTION_TARGETS: All production targets from all modules
   - ALL_TEST_TARGETS: All test targets from all modules
   - ALL_TARGETS: Combined list of both

STALENESS & WATCHING:

This repository is invalidated on changes to `.idea/modules.xml` via ctx.watch().
"""

load(":jps_model.bzl", "watch_project_model_files")

def _format_target_list(name, targets):
    """Format a list of targets as a Starlark list assignment."""
    if not targets:
        return "%s = []\n" % name

    lines = ["%s = [" % name]
    for target in targets:
        lines.append('    "%s",' % target)
    lines.append("]\n")
    return "\n".join(lines)

def _generate_targets_bzl(production_targets, test_targets, library_targets):
    """Generate the content for targets.bzl file."""
    content = []
    content.append(_format_target_list("ALL_PRODUCTION_COMMUNITY_TARGETS", production_targets))
    content.append(_format_target_list("ALL_TEST_COMMUNITY_TARGETS", test_targets))
    content.append(_format_target_list("ALL_LIBRARY_COMMUNITY_TARGETS", library_targets))
    content.append("ALL_COMMUNITY_TARGETS = ALL_PRODUCTION_COMMUNITY_TARGETS + ALL_TEST_COMMUNITY_TARGETS + ALL_LIBRARY_COMMUNITY_TARGETS")
    return "\n".join(content)

def _targets_repo_impl(ctx):
    # SETUP CONTEXT AND WATCHER
    root = ctx.path(Label("@community//:MODULE.bazel")).dirname

    # Watch the entire model to re-run generator on changes
    watch_project_model_files(ctx, root)

    script = (
        root
            .get_child("build")
            .get_child("jpsModelToBazelCommunityOnly.cmd")
    )

    # jps-to-bazel.cmd internally runs `bazel run` to execute the converter.
    # This "bazel inside bazel" works because repository rules execute during the loading phase,
    # before the current build's analysis phase starts. The inner bazel invocation is a completely
    # separate bazel server process that doesn't conflict with the outer one.
    if ctx.os.name.startswith("windows"):
        # proper quoting of the script path is important in the case of whitespace in the path, see https://ss64.com/nt/cmd.html
        res = ctx.execute(["cmd.exe", "/c", '""%s""' % script], quiet = False)
    else:
        res = ctx.execute(["/bin/bash", script], quiet = False)

    if res.return_code != 0:
        fail("jps-to-bazel failed (%d): %s" % (res.return_code, res.stderr))

    # READ AND PARSE JSON
    build_dir = root.get_child("build")
    targets_json = build_dir.get_child("bazel-targets.json")
    json_content = ctx.read(targets_json)
    targets_data = json.decode(json_content)

    all_production_targets = []
    all_test_targets = []
    all_library_targets = set()

    modules = targets_data.get("modules", {})
    for module_name in modules:
        module = modules[module_name]
        production_targets = module.get("productionTargets", [])
        test_targets = module.get("testTargets", [])

        for target in production_targets:
            if target not in all_production_targets:
                all_production_targets.append(target)

        for target in test_targets:
            if target not in all_test_targets:
                all_test_targets.append(target)

        module_libraries = module.get("moduleLibraries", {})
        for module_library_name in module_libraries:
            module_library = module_libraries[module_library_name]

            for jarTarget in module_library.get("jarTargets", []):
                all_library_targets.add(jarTarget)

    projectLibraries = targets_data.get("projectLibraries", {})
    for projectLibraryName in projectLibraries:
        projectLibrary = projectLibraries[projectLibraryName]
        for jarTarget in projectLibrary.get("jarTargets", []):
            all_library_targets.add(jarTarget)

    content = _generate_targets_bzl(all_production_targets, all_test_targets, all_library_targets)
    ctx.file("targets.bzl", content)

    # Expose it
    ctx.file("BUILD", 'exports_files(["targets.bzl"])')

targets_repo = repository_rule(
    implementation = _targets_repo_impl,
    # local = True,  # Rerun if filesystem changes
)

# Define the module extension that uses the repo rule
def _extension_impl(module_ctx):
    # Create the repository named "dynamic_deps"
    targets_repo(name = "jps_dynamic_deps_community")

# Export the extension
jps_dynamic_deps_community_extension = module_extension(
    implementation = _extension_impl,
)
